{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c587af4-ab01-4b0b-9342-599034dc128f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:40:54.1927238Z",
       "execution_start_time": "2025-06-26T11:40:50.814491Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "78784d56-309f-40d4-981b-8b798d74f0f9",
       "queued_time": "2025-06-26T11:40:40.5455827Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": "2025-06-26T11:40:40.546463Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "import synapse.ml.services\n",
    "from synapse.ml.services.language import AnalyzeText\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import sys\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39b103-97bc-4045-888f-0bbd0d8452a6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:40:57.5684463Z",
       "execution_start_time": "2025-06-26T11:40:57.2903943Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c699989d-c792-4d92-b296-95a4bc9dcb03",
       "queued_time": "2025-06-26T11:40:57.2892508Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  - Item id: 6b216ea0-70fd-46b0-8e99-6a3a5d0aa8c1\n",
      "  - Workspace id: d93a1ddb-3f94-4a2f-9b43-0afe4cdb9f17\n",
      "  - Analysis type: SentimentAnalysis\n",
      "  - Item Table: customer_feedback\n",
      "  - Source column: Feedback\n",
      "  - Result column: sentiment\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Setup\n",
    "#######################################################\n",
    "item_id = \"{ITEM_ID}\"\n",
    "item_workspace_id = \"{WORKSPACE_ID}\"\n",
    "analysis_type = \"SentimentAnalysis\"\n",
    "item_table_name = \"customer_feedback\"\n",
    "item_table_source_column_name = \"Feedback\"\n",
    "item_table_result_column_name = \"sentiment\"\n",
    "#item_id = spark_context.getConf().get(\"spark.itemId\")\n",
    "#item_workspace_id = spark_context.getConf().get(\"spark.itemWorkspaceId\")\n",
    "#analysis_type = spark_context.getConf().get(\"spark.analysisType\")\n",
    "#item_table_name = spark_context.getConf().get(\"spark.itemTableName\")\n",
    "#item_table_source_column_name = spark_context.getConf().get(\"spark.itemTableSourceColumnName\")\n",
    "#item_table_result_column_name = spark_context.getConf().get(\"spark.itemTableResultColumnName\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Item id: {item_id}\")\n",
    "print(f\"  - Workspace id: {item_workspace_id}\")\n",
    "print(f\"  - Analysis type: {analysis_type}\")\n",
    "print(f\"  - Item Table: {item_table_name}\")\n",
    "print(f\"  - Source column: {item_table_source_column_name}\")\n",
    "print(f\"  - Result column: {item_table_result_column_name}\")\n",
    "\n",
    "\n",
    "# Spark session builder\n",
    "spark_session = (SparkSession\n",
    "    .builder\n",
    "    .appName(analysis_type)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935dad81-b6e9-4f2f-a371-5b32e1ea671d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:41:10.0822525Z",
       "execution_start_time": "2025-06-26T11:41:05.1450019Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3ab1783e-15eb-4bb3-8dbe-32e07bb76f4d",
       "queued_time": "2025-06-26T11:41:05.1440193Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from table: customer_feedback\n",
      "Loaded 100 rows\n",
      "Available columns: Customer_ID, Feedback, sentiment\n",
      "Dropped existing result column: sentiment\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Load the Lakehouse table into a DataFrame\n",
    "#######################################################\n",
    "print(f\"Loading data from table: {item_table_name}\")\n",
    "df = spark_session.read.format(\"delta\").load(\"Tables/\" + item_table_name)\n",
    "\n",
    "print(f\"Loaded {df.count()} rows\")\n",
    "print(f\"Available columns: {', '.join(df.columns)}\")\n",
    "\n",
    "# Validate required column exists\n",
    "if item_table_source_column_name not in df.columns:\n",
    "    print(f\"Error: Source column '{item_table_source_column_name}' not found in table.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# drop the result column if it is there\n",
    "if item_table_result_column_name in df.columns:\n",
    "    df = df.drop(item_table_result_column_name)\n",
    "    print(f\"Dropped existing result column: {item_table_result_column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa3223f-f5a4-4b7e-9d81-3c55bd618418",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:43:56.009452Z",
       "execution_start_time": "2025-06-26T11:43:55.1833176Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b1f4a989-e736-4c0c-9beb-ef143b9441db",
       "queued_time": "2025-06-26T11:43:55.1822331Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SentimentAnalysis analysis on column: Feedback\n",
      "Transforming data with sentiment model...\n",
      "Finished transformation and obtained sentiment scores.\n",
      "Available columns in result: Customer_ID, Feedback, AnalyzeText_e1a6e058d5fa_error, TmpOutput\n",
      "Available column in analyze model: Customer_ID, Feedback, AnalyzeText_e1a6e058d5fa_error, TmpOutput\n",
      "Extracting sentiment values...\n",
      "Available columns in result: Customer_ID, Feedback, AnalyzeText_e1a6e058d5fa_error, TmpOutput, documents, sentiment\n",
      "Sentiment analysis complete, added column: sentiment\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Start  analysis\n",
    "#######################################################\n",
    "print(f\"Starting {analysis_type} analysis on column: {item_table_source_column_name}\")\n",
    "\n",
    "# Initialize the analysis model\n",
    "model = (AnalyzeText()\n",
    "        .setTextCol(item_table_source_column_name)\n",
    "        .setKind(analysis_type)\n",
    "        .setOutputCol(\"TmpOutput\"))\n",
    "\n",
    "# Apply the model to get sentiment scores\n",
    "print(\"Transforming data with sentiment model...\")\n",
    "result = model.transform(df)\n",
    "\n",
    "print(\"Finished transformation and obtained sentiment scores.\")\n",
    "print(f\"Available columns in result: {', '.join(result.columns)}\")\n",
    "\n",
    "print(f\"Available column in analyze model: {', '.join(result.columns)}\")\n",
    "\n",
    "# Extract the  values from the response\n",
    "if(analysis_type == \"SentimentAnalysis\"):\n",
    "    print(\"Extracting sentiment values...\")\n",
    "    result = result.withColumn(\"documents\", col(\"TmpOutput.documents\"))\\\n",
    "                    .withColumn(item_table_result_column_name, col(\"documents.sentiment\"))\n",
    "else:\n",
    "    print(f\"Error: Analysis type '{analysis_type}' not implemented.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(f\"Available columns in result: {', '.join(result.columns)}\")\n",
    "print(f\"Sentiment analysis complete, added column: {item_table_result_column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dc5cee-5f0a-47a7-89a9-d2807e082be8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:44:24.1021077Z",
       "execution_start_time": "2025-06-26T11:44:01.4332976Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a2ce467b-a55f-4273-a20b-3b7b44d6aa8e",
       "queued_time": "2025-06-26T11:44:01.4322628Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to: abfss://d93a1ddb-3f94-4a2f-9b43-0afe4cdb9f17@onelake.dfs.fabric.microsoft.com/6b216ea0-70fd-46b0-8e99-6a3a5d0aa8c1/Tables/customer_feedback/\n",
      "Output saved to table: customer_feedback\n",
      "+-----------+------------------------------------------+---------+\n",
      "|Customer_ID|Feedback                                  |sentiment|\n",
      "+-----------+------------------------------------------+---------+\n",
      "|101        |Horrible experience, will never return!   |negative |\n",
      "|102        |Customer service was rude and unhelpful.  |negative |\n",
      "|103        |The service was excellent, very satisfied!|positive |\n",
      "|104        |Would definitely come back again!         |neutral  |\n",
      "|105        |Would definitely come back again!         |neutral  |\n",
      "+-----------+------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Writing the result back to the Lakehouse table\n",
    "#######################################################\n",
    "# Get a list of all original columns from the input DataFrame\n",
    "original_columns = df.columns\n",
    "# Add the result column to the output\n",
    "if(item_table_result_column_name in original_columns):\n",
    "    output_df = result.select(*original_columns)\n",
    "else: \n",
    "    output_df = result.select(\n",
    "        *original_columns,   # All original columns using the * operator to unpack the list\n",
    "        col(item_table_result_column_name)  # Add the new result column\n",
    ")\n",
    "\n",
    "# Writing the data back\n",
    "deltaTablePath = f\"abfss://{item_workspace_id}@onelake.dfs.fabric.microsoft.com/{item_id}/Tables/{item_table_name}/\"\n",
    "print(f\"Writing results to: {deltaTablePath}\")\n",
    "output_df.write.mode(\"overwrite\").format(\"delta\").save(deltaTablePath)\n",
    "\n",
    "print(f\"Output saved to table: {item_table_name}\")\n",
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bea85cd-3523-4b50-a4d0-bca714b85760",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-26T11:47:17.5678397Z",
       "execution_start_time": "2025-06-26T11:47:09.7550446Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7e2d4633-c2ef-4f0a-bbad-fd87e698de20",
       "queued_time": "2025-06-26T11:47:09.7539778Z",
       "session_id": "2baf1853-a73d-4ca8-b3fb-a3eee47517a4",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 2baf1853-a73d-4ca8-b3fb-a3eee47517a4, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+\n",
      "|Customer_ID|            Feedback|sentiment|\n",
      "+-----------+--------------------+---------+\n",
      "|        101|Horrible experien...|     NULL|\n",
      "|        102|Customer service ...|     NULL|\n",
      "|        103|The service was e...|     NULL|\n",
      "|        104|Would definitely ...|     NULL|\n",
      "|        105|Would definitely ...|     NULL|\n",
      "+-----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the data\n",
    "\n",
    "df = spark_session.read.format(\"delta\").load(\"Tables/\" + item_table_name)\n",
    "\n",
    "if(item_table_result_column_name in df.columns):\n",
    "    df = df.drop(item_table_result_column_name)\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(deltaTablePath)\n",
    "\n",
    "df = spark_session.read.format(\"delta\").load(\"Tables/\" + item_table_name)\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "6b216ea0-70fd-46b0-8e99-6a3a5d0aa8c1",
    "default_lakehouse_name": "sentiment_test",
    "default_lakehouse_workspace_id": "d93a1ddb-3f94-4a2f-9b43-0afe4cdb9f17",
    "known_lakehouses": [
     {
      "id": "6b216ea0-70fd-46b0-8e99-6a3a5d0aa8c1"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
